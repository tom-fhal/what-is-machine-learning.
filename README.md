# Introduction au Projet “Qu’est-ce que le Machine Learning”
Bienvenue sur le projet “Qu’est-ce que le Machine Learning”. Ce projet est une exploration du Machine Learning, une partie importante de l’intelligence artificielle qui aide les ordinateurs à apprendre à partir de données. 

Le Machine Learning utilise des formules et des modèles pour trouver des tendances et faire des prédictions précises dans de nombreux domaines comme la santé ou la finance.

Avant de commencer les projets de Machine Learning, il est important de comprendre ce que c’est et de connaître différentes notions. Ce projet nous invites à faire des recherches sur plusieurs éléments clés du Machine Learning, comme la science des données, différents types d’apprentissage et de classification, la régression, la validation croisée, les données d’entraînement et de test, la corrélation entre deux variables, une fonction de coût, et la descente de gradient.


## La Science des Données
La science des données est un domaine interdisciplinaire qui utilise des méthodes, des processus et des systèmes pour extraire des connaissances ou des informations à partir de données. Elle implique des techniques et des théories tirées de plusieurs autres domaines plus larges des mathématiques, de la statistique, de l’informatique et de l’information.

Elle permet d’analyser des données massives pour en extraire des informations utiles, de réaliser des modèles et prédictions. La science des données a pris de l’ampleur avec l’augmentation exponentielle du volume de données numériques disponibles.

Références
https://www.cress-midipyrenees.org/definition-de-la-science-des-donnees/
https://www.ibm.com/fr-fr/topics/data-science


## L’apprentissage automatique

L’apprentissage automatique est une branche de l’intelligence artificielle qui utilise des données et des algorithmes pour imiter la manière dont les êtres humains apprennent, afin d’améliorer progressivement sa précision. Il s’agit d’un champ d’étude de l’IA qui se fonde sur des approches mathématiques et statistiques pour donner aux ordinateurs la capacité d’« apprendre » à partir de données. En pratique, on apprend en fait une hiérarchie de représentations, souvent les couches cachées de réseaux de neurones artificiels.

L’apprentissage profond
L’apprentissage profond est une technique de l’intelligence artificielle qui permet aux ordinateurs d’apprendre par eux-mêmes. Il utilise ce qu’on appelle des réseaux neuronaux, qui sont conçus pour imiter la façon dont le cerveau humain fonctionne.

Ces réseaux neuronaux sont composés de plusieurs couches, chacune traitant une partie différente de l’information. C’est un peu comme si chaque couche de neurones était une classe dans une école, où chaque classe se concentre sur un sujet différent.

En utilisant ces réseaux neuronaux et leurs différentes couches, l’apprentissage profond peut résoudre des tâches complexes en transformant les données d’entrée (comme des images ou du texte) en une forme que l’ordinateur peut comprendre.

Cela fait partie d’une plus grande famille de techniques appelées apprentissage automatique, qui sont toutes basées sur l’idée que les ordinateurs peuvent apprendre à partir de données.

https://fr.wikipedia.org/wiki/Apprentissage_automatique
https://www.ibm.com/fr-fr/topics/machine-learning

https://www.lebigdata.fr/deep-learning-definition
https://www.wikiwand.com/fr/Apprentissage_profond
https://www.cnil.fr/fr/definition/apprentissage-profond-deep-learning


## L’apprentissage supervisé
L’apprentissage supervisé est une méthode de l’intelligence artificielle qui utilise des données et des algorithmes pour imiter la manière dont les êtres humains apprennent, afin d’améliorer progressivement sa précision. Il s’agit d’un champ d’étude de l’IA qui se fonde sur des approches mathématiques et statistiques pour donner aux ordinateurs la capacité d’« apprendre » à partir de données. En pratique, on apprend en fait une hiérarchie de représentations, souvent les couches cachées de réseaux de neurones artificiels.

L’apprentissage supervisé est également appelé apprentissage automatique supervisé, et se caractérise par l’utilisation de jeux de données étiquetés qui entraînent des algorithmes permettant de classer des données ou de prédire des résultats avec précision. Au fur et à mesure que les données en entrée sont introduites dans le modèle, celui-ci ajuste ses pondérations jusqu’à ce que le modèle soit correctement ajusté.

https://blent.ai/blog/a/apprentissage-supervise-definition
https://www.wikiwand.com/fr/Apprentissage_supervis%C3%A9

## L'apprentissage non supervisé
L’apprentissage non supervisé est une méthode d’intelligence artificielle qui utilise des algorithmes pour analyser et regrouper des jeux de données non étiquetés1. Ces algorithmes découvrent des modèles cachés ou des groupements de données sans nécessiter d’intervention humaine1. Sa capacité à découvrir les similitudes et les différences d’informations en fait la solution idéale pour l’analyse d’exploration des données.

Il existe deux principales méthodes d’apprentissage non supervisées:

Les méthodes par partitionnement telles que les algorithmes des k-moyennes ou k-médoïdes.
La classification hiérarchique, ou classification ascendante hiérarchique (CAH), est un algorithme de classification non supervisée qui peut être agglomérative ou divisive.
Références
https://www.techopedia.com/fr/dictionnaire/apprentissage-non-supervise
https://www.journaldunet.fr/intelligence-artificielle/guide-de-l-intelligence-artificielle/1501309-apprentissage-non-supervise/
https://www.ibm.com/fr-fr/topics/unsupervised-learning

##  La classification supervisé 
La classification supervisée est une méthode d’apprentissage automatique qui consiste à attribuer automatiquement une catégorie (ou une classe) à des données dont on ne connaît pas la catégorie. Pour ce faire, un classifieur (algorithme de machine learning) est entraîné sur des données similaires ou très proches des données que l’on souhaite classer.

Elle est largement utilisée et bien maîtrisée industriellement, permettant de résoudre un large panel de problèmes pratiques de la vie réelle tels que la détection de défaut d’usinage, de fraude, de maladie, le tri automatique de courrier, de document ou encore de vidéo, la reconnaissance d’images (ex : reconnaissance de caractères, plantes, pollens, etc…).

Références
https://fr.wikipedia.org/wiki/Classement_automatique
https://kongakura.fr/article/Classification%20supervis%C3%A9e

##  La classification non supervisée
La classification non supervisée est une méthode d’apprentissage automatique où les données ne sont pas étiquetées. C’est-à-dire que l’algorithme doit découvrir par lui-même les structures sous-jacentes à ces données non étiquetées.

Elle consiste à regrouper des données non étiquetées en fonction de leurs similitudes ou de leurs différences. Les algorithmes de classification non supervisée sont utilisés pour traiter des objets de données bruts et non classés en groupes représentés par des structures ou des modèles dans les informations.

Il existe deux principales méthodes de classification non supervisée:

Les méthodes par partitionnement telles que les algorithmes des k-moyennes ou k-médoïdes.
La classification hiérarchique, ou classification ascendante hiérarchique (CAH), est un algorithme de classification non supervisée qui peut être agglomérative ou divisive.
Références
https://fr.wikipedia.org/wiki/Apprentissage_non_supervis%C3%A9
https://www.ibm.com/fr-fr/topics/unsupervised-learning
https://eo.belspo.be/fr/classification-non-supervisee#:~:text=Classification%20non-supervis%C3%A9e%20Dans%20cette%20approche%2C%20on%20laisse%20l%27ordinateur,les%20pixels%20sur%20base%20de%20signatures%20spectrales%20similaires.

##  La régression  
La régression en intelligence artificielle est une méthode d’apprentissage supervisé qui permet de prédire une variable continue à partir d’une ou plusieurs autres variables. Par exemple, prédire le prix d’une maison en fonction de sa taille et de son emplacement est un problème de régression.

Il existe plusieurs types de régression, dont la régression linéaire et la régression logistique. La régression linéaire est utilisée lorsque la relation entre les variables d’entrée et la variable de sortie est supposée être linéaire. La régression logistique, en revanche, est utilisée pour prédire une variable de sortie binaire (par exemple, oui/non ou vrai/faux) à partir d’une ou plusieurs variables d’entrée.
Références
https://www.journaldunet.fr/intelligence-artificielle/guide-de-l-intelligence-artificielle/1501891-regression-lineaire-definition-fonctionnement-et-interpretation/
https://mailchimp.com/fr/resources/machine-learning-regression/

## La validation croisée 
La validation croisée, ou “cross-validation” en anglais, est une méthode statistique utilisée pour évaluer les performances d’un modèle d’apprentissage automatique. Elle est particulièrement utile lorsque vous disposez de données limitées.

Dans la validation croisée, l’ensemble de données est divisé en plusieurs sous-ensembles. Un sous-ensemble est utilisé comme données de test et les autres sous-ensembles sont utilisés comme données d’entraînement. Le processus de validation est répété pendant plusieurs tours, avec chaque sous-ensemble utilisé exactement une fois comme données de test.

L’objectif principal de la validation croisée est de tester la capacité du modèle à généraliser à des données indépendantes. Elle aide également à éviter le surapprentissage, un problème courant en apprentissage automatique où un modèle s’adapte trop bien aux données d’entraînement et fonctionne mal sur les nouvelles données.

Références
https://datascientest.com/cross-validation

## Les données d’entraînement, les données de test et/ ou de validation

### Les données d’entraînement
Les données d’entraînement sont un ensemble initial de données utilisées pour aider un programme à comprendre comment appliquer des technologies comme les réseaux de neurones pour apprendre et produire des résultats sophistiqués. Elles sont utilisées pour entraîner le modèle et lui permettre ensuite de réaliser des prédictions sur la base de nouvelles données.

### Les données de test
Les données de test sont utilisées pour évaluer la performance d’un modèle une fois qu’il a été formé. Elles sont distinctes des données d’entraînement et ne sont pas utilisées pendant la phase d’entraînement. L’objectif est de tester la capacité du modèle à généraliser à de nouvelles données.

### Les données de validation
Les données de validation sont utilisées pendant la phase d’entraînement d’un modèle pour évaluer sa performance et ajuster ses paramètres. Elles permettent de vérifier si le modèle est en train de surapprendre (c’est-à-dire qu’il se comporte bien sur les données d’entraînement mais mal sur les données non vues). Les données de validation sont généralement tirées du même ensemble de données que les données d’entraînement, mais elles ne sont pas utilisées directement pour entraîner le modèle.

Références
https://fr.wikipedia.org/wiki/Validation_de_donn%C3%A9es
https://www.journaldunet.fr/intelligence-artificielle/guide-de-l-intelligence-artificielle/1501329-data-preparation-en-machine-learning-definition/
https://www.lebigdata.fr/machine-learning-entrainement-ia

## Corrélation linéaire (de Pearson) entre deux variables

La corrélation linéaire de Pearson, aussi appelée r de Pearson, est une mesure statistique qui exprime à quel point deux variables sont linéairement liées. Par exemple, on peut l’utiliser pour voir si l’âge d’une personne a une relation avec son salaire.

Le coefficient de corrélation de Pearson varie entre -1 et +1. Un r de Pearson de +1 indique une corrélation linéaire positive parfaite entre les deux variables, tandis qu’un r de Pearson de -1 indique une corrélation linéaire négative parfaite. Une valeur de 0 signifie qu’il n’y a pas de corrélation linéaire entre les variables.

La corrélation linéaire de Pearson est un outil précieux en statistiques car elle peut nous aider à comprendre la relation entre différentes variables. Par exemple, elle peut être utilisée en économie pour comprendre comment les changements dans une variable (comme le taux de chômage) peuvent affecter une autre variable (comme l’inflation).

Références : 
https://fr.wikipedia.org/wiki/Corr%C3%A9lation_(statistiques)
http://grasland.script.univ-paris-diderot.fr/STAT98/stat98_6/stat98_6.htm
https://www.questionpro.com/blog/fr/coefficient-de-correlation-de-pearson/#:~:text=Le%20coefficient%20de%20corrélation%20produit,penchera%20vers%201%20ou%20%2D1.

## Fonction de coût en Machine Learnings

La fonction de coût, en Machine Learning, est une mesure qui nous permet de savoir à quel point notre modèle fait des erreurs. Elle compare les prédictions du modèle avec les vraies valeurs pour quantifier l’erreur.


La fonction de coût prend en compte les prédictions du modèle et les vraies valeurs, puis calcule une sorte de “distance” entre elles. Cette “distance” est l’erreur que notre modèle a commise. Plus cette erreur est petite, mieux c’est.

La fonction de coût est essentielle car elle guide l’apprentissage du modèle. En essayant de minimiser cette fonction de coût, le modèle s’ajuste pour faire de meilleures prédictions.

Références : 
https://builtin.com/machine-learning/cost-function#:~:text=Is%20Cost%20Function%3F-,Cost%20function%20measures%20the%20performance%20of%20a%20machine%20learning%20model,of%20a%20single%20real%20number.
https://www.analyticsvidhya.com/blog/2021/02/cost-function-is-no-rocket-science/
https://www.javatpoint.com/cost-function-in-machine-learning

## La descente de gradient en Data Science
La descente de gradient est une technique d’optimisation utilisée pour minimiser une fonction, généralement la fonction de coût dans le contexte de la Data Science.

La descente de gradient fonctionne en ajustant itérativement les paramètres d’un modèle pour minimiser la fonction de coût. L’ajustement est effectué en déplaçant les paramètres dans la direction opposée au gradient de la fonction de coût, d’où le nom “descente de gradient”. Le taux à lequel les paramètres sont ajustés est contrôlé par un paramètre appelé taux d’apprentissage.

 La descente de gradient est un composant clé de nombreux algorithmes de Machine Learning. En minimisant la fonction de coût, la descente de gradient permet d’améliorer la précision des prédictions du modèle. C’est un outil essentiel pour l’apprentissage supervisé et non supervisé.

Références : 
https://builtin.com/data-science/gradient-descent
https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21
